{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hotel Review Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\vishw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vishw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "string.punctuation\n",
    "import re\n",
    "from textblob import TextBlob # using text blob function\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "import gensim\n",
    "from pattern.text.en import singularize\n",
    "punctuations = '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~'\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "#NLP Pre-Process:\n",
    "def entier_func(directory,data,column_drop,col_name,hotel=0):\n",
    "    os.chdir(directory)\n",
    "    os.getcwd()\n",
    "    \n",
    "    df = pd.read_csv(data)\n",
    "    df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "    df.dropna(axis=0,inplace=True)\n",
    "    if hotel == 0:\n",
    "        df1 = df.drop(column_drop,axis=1)\n",
    "    else:\n",
    "        df1 = df[df['hotel_name'] == hotel]\n",
    "        df1 = df1.drop(column_drop,axis=1)\n",
    "    # pre-processing:\n",
    "    \n",
    "    # punctuation removal\n",
    "    def remove_punctuation(text):\n",
    "        punctuation_free=\"\".join([i for i in text if i not in punctuations])\n",
    "        return punctuation_free\n",
    "    \n",
    "    df1['Review_no_punc']=df1[col_name].apply(lambda x: remove_punctuation(x))\n",
    "    \n",
    "    # lowering text\n",
    "    df1['Review_lower']= df1['Review_no_punc'].apply(lambda x: x.lower())\n",
    "    \n",
    "    # extra white space removal\n",
    "    df1['Review_no_extra_space'] = df1['Review_lower'].apply(lambda text: re.sub(' +', ' ', text))\n",
    "    \n",
    "    # emoji removal\n",
    "    def emoji(string):  # created a emoji removing funcction\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', string)\n",
    "    \n",
    "    df1['Review_no_emoji'] = df1['Review_no_extra_space'].apply(emoji)\n",
    "    \n",
    "    ## ------------- For compactness pruning and redundancy pruning----------------\n",
    "    # Sentence Tokenization:\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    df1['Review_sent_token'] = df1['Review_no_emoji'].apply(sent_tokenize)\n",
    "        \n",
    "    # stop words removal:\n",
    "    def stop_words_removal(text):\n",
    "        result = []\n",
    "        for token in gensim.utils.simple_preprocess(text):\n",
    "            if token not in stop_words:\n",
    "                result.append(token)\n",
    "        return result\n",
    "    \n",
    "    b = df1['Review_no_emoji'].apply(stop_words_removal)\n",
    "    df1['Stop_words_removal'] = b.apply(lambda x: \" \".join(x))\n",
    "    ## -----------------------------------------------------------------------------\n",
    "    \n",
    "    # word Tokenization:\n",
    "    def tokenization(text):\n",
    "        tokens = re.split('W+',text)\n",
    "        return tokens\n",
    "    \n",
    "    a = df1['Review_no_emoji'].apply(lambda x: tokenization(x)) # calling the function\n",
    "    df1['Review_token'] = a.apply(lambda x: \" \".join(x)) # Join the words into the String\n",
    "    \n",
    "    # remove the words less than two words:\n",
    "    def greater_than_2(text):\n",
    "        result = []\n",
    "        for token in gensim.utils.simple_preprocess(text):\n",
    "            if len(token) > 3:\n",
    "                result.append(token)\n",
    "        return result\n",
    "    \n",
    "    b = df1['Review_token'].apply(lambda x: greater_than_2(x)) \n",
    "    df1['Review_above_3token'] = b.apply(lambda x: \" \".join(x)) # Join the words into the String\n",
    "    # Necessary Tagging:\n",
    "    \n",
    "    # parts of speech tagging\n",
    "    def pos_tag(text):\n",
    "        try:\n",
    "            return TextBlob(text).tags\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    df1['Review_pos_tag'] = df1['Review_above_3token'].apply((pos_tag)) # calling pos_tag function \n",
    "    \n",
    "    # addjective tagging\n",
    "    def get_adjectives(text):\n",
    "        blob = TextBlob(text)\n",
    "        return [ word for (word,tag) in blob.tags if tag == \"JJ\"]\n",
    "    \n",
    "    df1['Review_adjective'] = df1['Review_above_3token'].apply(get_adjectives)\n",
    "    \n",
    "    # Common Noun(NN and NNS) [which is called Noun and as well as common noun and NNP, NNPS is called proper Noun]\n",
    "    def get_common_nouns(text):\n",
    "        blob = TextBlob(text)\n",
    "        return [ word for (word,tag) in blob.tags if tag == \"NN\" or tag == \"NNS\"]\n",
    "    \n",
    "    df1['Review_Common_Noun'] = df1['Review_above_3token'].apply(get_common_nouns)\n",
    "    df1.reset_index(inplace = True, drop = True) # changed the index structure\n",
    "    \n",
    "    # converting it into singular\n",
    "    def singularity(texts):\n",
    "        return [singularize(text) for text in texts]\n",
    "    # calling the function and saving it as like singular noun\n",
    "    df1['Review_Singular_Cnoun'] = df1['Review_Common_Noun'].apply(lambda x: singularity(x)) # calling the function\n",
    "    print(df1.shape)\n",
    "    return df1\n",
    "\n",
    "# Binary Conversion:\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "def binary_conversion(data,colname):\n",
    "    mlb = MultiLabelBinarizer() # binary converter\n",
    "    data = pd.DataFrame(mlb.fit_transform(df1[data]),columns=mlb.classes_)\n",
    "    \n",
    "    # getting only unique noun list(which is used in binary columns)\n",
    "    unique_noun_list = []   # it is for unique words\n",
    "    for i in range(0,len(df1[colname])): # iterating the len of noun column\n",
    "        for j in df1[colname][i]:        # iterating the text of each index in noun column\n",
    "            if j not in unique_noun_list:      # if the iterated text not presented inside the list\n",
    "                unique_noun_list.append(j)     # then we append it into the list\n",
    "            else:                              # if not we leave it and continue\n",
    "                continue\n",
    "    print(\"Unique Noun Lists: \")\n",
    "    print(\"Showing the sample output: \",unique_noun_list[0:10])\n",
    "    return data\n",
    "\n",
    "# Association Rule\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "def association_rule(data,dup_col1,dup_col2):\n",
    "    df2 = data.copy()\n",
    "    #using Apriori rule to find the frequent set\n",
    "    freq_reviewset = apriori(df = df2, min_support = 0.03, use_colnames=True) # fixing the support as 0.03\n",
    "    freq_reviewset.to_csv('frequent_review_set.csv', encoding='utf-8')\n",
    "    \n",
    "    rules = association_rules(freq_reviewset,min_threshold=0.2) #fixing threshold as 0.2\n",
    "    rules.to_csv('association_rule.csv', encoding='utf-8')\n",
    "    \n",
    "    rules.drop_duplicates(subset=[dup_col1,dup_col2],keep='first',inplace=True)\n",
    "    return rules\n",
    "\n",
    "\n",
    "# Finding the max support and removing the duplicates\n",
    "\n",
    "def max_support_finding(rules,one_of_asso_column):\n",
    "    rules1 = rules[['antecedents','consequents','support']]\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    max = []\n",
    "    \n",
    "    df2 = rules1\n",
    "    dfs =  pd.DataFrame()\n",
    "    dbs =  pd.DataFrame()\n",
    "    \n",
    "    for x in range(len(df2)):\n",
    "        l1.append(df2.iloc[x,0])\n",
    "        l2.append(df2.iloc[x,1])\n",
    "    \n",
    "    for x in range(len(l1)):\n",
    "        for y in range(len(df2)):\n",
    "            if (l1[x] == df2.iloc[y,1]) and (l2[x] == df2.iloc[y,0]):\n",
    "                kd = (df2.iloc[[x],:])\n",
    "                dfs = pd.concat([dfs,kd],axis = 0)\n",
    "                dfs.sort_values(by =one_of_asso_column, ascending = False)\n",
    "    \n",
    "    f = list(dfs.index)\n",
    "\n",
    "    dfnon = df2.drop(f)\n",
    "    \n",
    "\n",
    "    for x in range(len(dfs)):\n",
    "        if x % 2 == 0:\n",
    "            s = dfs.iloc[x:x+2 , 0:2].max()\n",
    "            max.append(dfs.iloc[x:x+2,2].max())\n",
    "            #print(s)\n",
    "            dbs = pd.concat([dbs,s],axis = 1)\n",
    "    \n",
    "    dbs = dbs.T.reset_index(drop=True)\n",
    "    \n",
    "    dbs[one_of_asso_column] = max\n",
    "\n",
    "    new_data = pd.concat([dfnon , dbs])\n",
    "\n",
    "    new_data.reset_index(drop=True,inplace = True)\n",
    "\n",
    "    return (new_data)\n",
    "\n",
    "\n",
    "#Compactness Pruning\n",
    "def compact_pruning():\n",
    "    prun_data = new_data.copy()\n",
    "\n",
    "    # converting the frozenset to list\n",
    "    prun_data['antecedents'] = prun_data[['antecedents']].applymap(lambda x: list(x))\n",
    "    prun_data['consequents'] = prun_data[['consequents']].applymap(lambda x: list(x))\n",
    "\n",
    "    # conversion from list to str\n",
    "    prun_data['antecedents'] = prun_data[['antecedents']].applymap(lambda x: \" \".join(x))\n",
    "    prun_data['consequents'] = prun_data[['consequents']].applymap(lambda x: \" \".join(x))\n",
    "\n",
    "    comp = []\n",
    "    first = []\n",
    "    second = []\n",
    "    count=0\n",
    "    k=0\n",
    "    mindis = []\n",
    "    count = 0\n",
    "    feat = []\n",
    "    w = []\n",
    "\n",
    "    s = []\n",
    "    for i in df1['Review_sent_token']:\n",
    "        for j in i:\n",
    "            s.append(j.split( ))\n",
    "\n",
    "    for i,j in zip(prun_data['antecedents'],prun_data['consequents']):\n",
    "        feat.append(i)\n",
    "        feat.append(j)\n",
    "        #print(feat)\n",
    "        for i in s:\n",
    "            for word in i: \n",
    "                if word not in stop_words:\n",
    "                    w.append(word)\n",
    "            #print(w)\n",
    "            if (feat[0] in w and feat[1] in w):\n",
    "                #print(w)\n",
    "                for j in w:\n",
    "                    #print(j)\n",
    "                    if j == feat[0]:\n",
    "                        #print(j)\n",
    "                        k =w.index(feat[0],k)\n",
    "                        #print(k)\n",
    "                        first.append(k)\n",
    "                        k += 1\n",
    "                k = 0\n",
    "                for j in w:\n",
    "                    #print(j)\n",
    "                    if(j==feat[1]):\n",
    "                        k = w.index(feat[1],k)\n",
    "                        second.append(k)\n",
    "                        k += 1\n",
    "                for i in first:\n",
    "                    for j in second:\n",
    "                        if(i<j):\n",
    "                            mindis.append((j-i)-1)\n",
    "                    #print(mindis)\n",
    "                    if(mindis == []):\n",
    "                        break\n",
    "                    if (min(mindis)) <= 5:\n",
    "                        #print(min(mindis))\n",
    "                        count+= 1\n",
    "                    #print(count)\n",
    "                k = 0\n",
    "            w = []\n",
    "            first = []\n",
    "            second = []\n",
    "            mindis = []\n",
    "        if (count >= 2):\n",
    "                comp.append(feat)\n",
    "        feat = []\n",
    "        count = 0\n",
    "    df_comp = pd.DataFrame(comp)\n",
    "    df_comp = df_comp.rename(columns = {0:'antecedents',1:'consequents'}, inplace = False)\n",
    "    \n",
    "    return (comp)\n",
    "\n",
    "# Redundancy Pruning\n",
    "\n",
    "# 1.two feature feature for redundancy pruning:\n",
    "def twofeats():\n",
    "    twofeat = {}\n",
    "    w = []\n",
    "    feat = []\n",
    "    k = 0\n",
    "    count = 0\n",
    "    s = []\n",
    "    for i in df1['Review_sent_token']:\n",
    "        for j in i:\n",
    "            s.append(j.split( ))\n",
    "    for i,j in comp:\n",
    "        feat.append(i)\n",
    "        feat.append(j)\n",
    "        #print(feat)\n",
    "        for i in s:\n",
    "            for word in i: \n",
    "                if word not in stop_words:\n",
    "                    w.append(word)\n",
    "            #print(w)\n",
    "            if (feat[0] in w and feat[1] in w):\n",
    "                #print(w)\n",
    "                for j in w:\n",
    "                    #print(j)\n",
    "                    if j == feat[0]:\n",
    "                        #print(j)\n",
    "                        k =w.index(feat[0],k)\n",
    "                        #print(k)\n",
    "                        first.append(k)\n",
    "                        k += 1\n",
    "                k = 0\n",
    "                for j in w:\n",
    "                    #print(j)\n",
    "                    if(j==feat[1]):\n",
    "                        k = w.index(feat[1],k)\n",
    "                        second.append(k)\n",
    "                        k += 1\n",
    "                for i in first:\n",
    "                    for j in second:\n",
    "                        if(i<j):\n",
    "                            mindis.append((j-i)-1)\n",
    "                    #print(mindis)\n",
    "                    if(mindis == []):\n",
    "                        break\n",
    "                    if (min(mindis)) <= 5:\n",
    "                        #print(min(mindis))\n",
    "                        count+= 1\n",
    "                    #print(count)\n",
    "                k = 0\n",
    "            w = []\n",
    "            first = []\n",
    "            second = []\n",
    "            mindis = []\n",
    "        #print(feat)\n",
    "        twofeat[feat[0],feat[1]] = count\n",
    "        feat = []\n",
    "        #print(count)\n",
    "        count = 0\n",
    "    return twofeat\n",
    "\n",
    "# 2. single feature for redundancy pruning:\n",
    "def single_feat():\n",
    "    count = 0\n",
    "    sinfeat = {}\n",
    "    s = []\n",
    "    for i in df1['Review_sent_token']:\n",
    "        for j in i:\n",
    "            s.append(j.split( ))\n",
    "    for l in np.unique(comp):\n",
    "        for i in s:\n",
    "            for j in i:\n",
    "                if(j == l):\n",
    "                    count+= 1\n",
    "        #print(l)\n",
    "        #print(count)\n",
    "        sinfeat[l] = count\n",
    "        count = 0\n",
    "    return sinfeat\n",
    "# 3. p support redundancy pruning:\n",
    "def p_support(twofeat,sinfeat):\n",
    "    sum2 = 0\n",
    "    support = 0\n",
    "    red = {}\n",
    "    for j in sinfeat:\n",
    "        #print(j)\n",
    "        for i in twofeat:\n",
    "            #print(i)\n",
    "            if(j in i):\n",
    "                sum2 += twofeat[i]\n",
    "        support = sinfeat[j] - sum2\n",
    "        red[j] = support\n",
    "        support = 0\n",
    "        sum2 = 0\n",
    "    return red\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Pre-Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 Hotel Names:\n",
    "- Ocean Palms Goa\n",
    "- Silver Sands Serenity\n",
    "- Goa Woodlands Hotel\n",
    "- The Byke Old Anchor Beach Resort & Spa\n",
    "- Whispering Palms Beach Resort\n",
    "- Rivasa Resort\n",
    "- Rendezvous Beach Resort\n",
    "- Hotel Campal\n",
    "- Hotel MR Manfred\n",
    "- Hotel Royal Palace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(773, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_no_punc</th>\n",
       "      <th>Review_lower</th>\n",
       "      <th>Review_no_extra_space</th>\n",
       "      <th>Review_no_emoji</th>\n",
       "      <th>Review_sent_token</th>\n",
       "      <th>Stop_words_removal</th>\n",
       "      <th>Review_token</th>\n",
       "      <th>Review_above_3token</th>\n",
       "      <th>Review_pos_tag</th>\n",
       "      <th>Review_adjective</th>\n",
       "      <th>Review_Common_Noun</th>\n",
       "      <th>Review_Singular_Cnoun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toilet was not cleaned on arrival. Substandard...</td>\n",
       "      <td>Toilet was not cleaned on arrival. Substandard...</td>\n",
       "      <td>toilet was not cleaned on arrival. substandard...</td>\n",
       "      <td>toilet was not cleaned on arrival. substandard...</td>\n",
       "      <td>toilet was not cleaned on arrival. substandard...</td>\n",
       "      <td>[toilet was not cleaned on arrival., substanda...</td>\n",
       "      <td>toilet cleaned arrival substandard service rep...</td>\n",
       "      <td>toilet was not cleaned on arrival. substandard...</td>\n",
       "      <td>toilet cleaned arrival substandard service rep...</td>\n",
       "      <td>[(toilet, NN), (cleaned, VBD), (arrival, JJ), ...</td>\n",
       "      <td>[arrival, substandard, multiple, want, good, g...</td>\n",
       "      <td>[toilet, service, reponse, times, things, aver...</td>\n",
       "      <td>[toilet, service, reponse, time, thing, averag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cleanliness is Average . . Spider webs on the ...</td>\n",
       "      <td>Cleanliness is Average . . Spider webs on the ...</td>\n",
       "      <td>cleanliness is average . . spider webs on the ...</td>\n",
       "      <td>cleanliness is average . . spider webs on the ...</td>\n",
       "      <td>cleanliness is average . . spider webs on the ...</td>\n",
       "      <td>[cleanliness is average ., ., spider webs on t...</td>\n",
       "      <td>cleanliness average spider webs room wall chec...</td>\n",
       "      <td>cleanliness is average . . spider webs on the ...</td>\n",
       "      <td>cleanliness average spider webs room wall chec...</td>\n",
       "      <td>[(cleanliness, JJ), (average, JJ), (spider, NN...</td>\n",
       "      <td>[cleanliness, average, half, good, tasty, over...</td>\n",
       "      <td>[spider, webs, room, wall, check, delay, hour,...</td>\n",
       "      <td>[spider, web, room, wall, check, delay, hmy, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OK stay.</td>\n",
       "      <td>OK stay.</td>\n",
       "      <td>ok stay.</td>\n",
       "      <td>ok stay.</td>\n",
       "      <td>ok stay.</td>\n",
       "      <td>[ok stay.]</td>\n",
       "      <td>ok stay</td>\n",
       "      <td>ok stay.</td>\n",
       "      <td>stay</td>\n",
       "      <td>[(stay, NN)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[stay]</td>\n",
       "      <td>[stay]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good central place in south goa.</td>\n",
       "      <td>Good central place in south goa.</td>\n",
       "      <td>good central place in south goa.</td>\n",
       "      <td>good central place in south goa.</td>\n",
       "      <td>good central place in south goa.</td>\n",
       "      <td>[good central place in south goa.]</td>\n",
       "      <td>good central place south goa</td>\n",
       "      <td>good central place in south goa.</td>\n",
       "      <td>good central place south</td>\n",
       "      <td>[(good, JJ), (central, JJ), (place, NN), (sout...</td>\n",
       "      <td>[good, central]</td>\n",
       "      <td>[place, south]</td>\n",
       "      <td>[place, south]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business Visit to Goa, Venue is in perfect loc...</td>\n",
       "      <td>Business Visit to Goa Venue is in perfect loca...</td>\n",
       "      <td>business visit to goa venue is in perfect loca...</td>\n",
       "      <td>business visit to goa venue is in perfect loca...</td>\n",
       "      <td>business visit to goa venue is in perfect loca...</td>\n",
       "      <td>[business visit to goa venue is in perfect loc...</td>\n",
       "      <td>business visit goa venue perfect location hear...</td>\n",
       "      <td>business visit to goa venue is in perfect loca...</td>\n",
       "      <td>business visit venue perfect location heart ci...</td>\n",
       "      <td>[(business, NN), (visit, NN), (venue, NN), (pe...</td>\n",
       "      <td>[perfect, good, various, comfortable, good, go...</td>\n",
       "      <td>[business, visit, venue, location, heart, city...</td>\n",
       "      <td>[busines, visit, venue, location, heart, city,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  \\\n",
       "0  Toilet was not cleaned on arrival. Substandard...   \n",
       "1  Cleanliness is Average . . Spider webs on the ...   \n",
       "2                                           OK stay.   \n",
       "3                   Good central place in south goa.   \n",
       "4  Business Visit to Goa, Venue is in perfect loc...   \n",
       "\n",
       "                                      Review_no_punc  \\\n",
       "0  Toilet was not cleaned on arrival. Substandard...   \n",
       "1  Cleanliness is Average . . Spider webs on the ...   \n",
       "2                                           OK stay.   \n",
       "3                   Good central place in south goa.   \n",
       "4  Business Visit to Goa Venue is in perfect loca...   \n",
       "\n",
       "                                        Review_lower  \\\n",
       "0  toilet was not cleaned on arrival. substandard...   \n",
       "1  cleanliness is average . . spider webs on the ...   \n",
       "2                                           ok stay.   \n",
       "3                   good central place in south goa.   \n",
       "4  business visit to goa venue is in perfect loca...   \n",
       "\n",
       "                               Review_no_extra_space  \\\n",
       "0  toilet was not cleaned on arrival. substandard...   \n",
       "1  cleanliness is average . . spider webs on the ...   \n",
       "2                                           ok stay.   \n",
       "3                   good central place in south goa.   \n",
       "4  business visit to goa venue is in perfect loca...   \n",
       "\n",
       "                                     Review_no_emoji  \\\n",
       "0  toilet was not cleaned on arrival. substandard...   \n",
       "1  cleanliness is average . . spider webs on the ...   \n",
       "2                                           ok stay.   \n",
       "3                   good central place in south goa.   \n",
       "4  business visit to goa venue is in perfect loca...   \n",
       "\n",
       "                                   Review_sent_token  \\\n",
       "0  [toilet was not cleaned on arrival., substanda...   \n",
       "1  [cleanliness is average ., ., spider webs on t...   \n",
       "2                                         [ok stay.]   \n",
       "3                 [good central place in south goa.]   \n",
       "4  [business visit to goa venue is in perfect loc...   \n",
       "\n",
       "                                  Stop_words_removal  \\\n",
       "0  toilet cleaned arrival substandard service rep...   \n",
       "1  cleanliness average spider webs room wall chec...   \n",
       "2                                            ok stay   \n",
       "3                       good central place south goa   \n",
       "4  business visit goa venue perfect location hear...   \n",
       "\n",
       "                                        Review_token  \\\n",
       "0  toilet was not cleaned on arrival. substandard...   \n",
       "1  cleanliness is average . . spider webs on the ...   \n",
       "2                                           ok stay.   \n",
       "3                   good central place in south goa.   \n",
       "4  business visit to goa venue is in perfect loca...   \n",
       "\n",
       "                                 Review_above_3token  \\\n",
       "0  toilet cleaned arrival substandard service rep...   \n",
       "1  cleanliness average spider webs room wall chec...   \n",
       "2                                               stay   \n",
       "3                           good central place south   \n",
       "4  business visit venue perfect location heart ci...   \n",
       "\n",
       "                                      Review_pos_tag  \\\n",
       "0  [(toilet, NN), (cleaned, VBD), (arrival, JJ), ...   \n",
       "1  [(cleanliness, JJ), (average, JJ), (spider, NN...   \n",
       "2                                       [(stay, NN)]   \n",
       "3  [(good, JJ), (central, JJ), (place, NN), (sout...   \n",
       "4  [(business, NN), (visit, NN), (venue, NN), (pe...   \n",
       "\n",
       "                                    Review_adjective  \\\n",
       "0  [arrival, substandard, multiple, want, good, g...   \n",
       "1  [cleanliness, average, half, good, tasty, over...   \n",
       "2                                                 []   \n",
       "3                                    [good, central]   \n",
       "4  [perfect, good, various, comfortable, good, go...   \n",
       "\n",
       "                                  Review_Common_Noun  \\\n",
       "0  [toilet, service, reponse, times, things, aver...   \n",
       "1  [spider, webs, room, wall, check, delay, hour,...   \n",
       "2                                             [stay]   \n",
       "3                                     [place, south]   \n",
       "4  [business, visit, venue, location, heart, city...   \n",
       "\n",
       "                               Review_Singular_Cnoun  \n",
       "0  [toilet, service, reponse, time, thing, averag...  \n",
       "1  [spider, web, room, wall, check, delay, hmy, r...  \n",
       "2                                             [stay]  \n",
       "3                                     [place, south]  \n",
       "4  [busines, visit, venue, location, heart, city,...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLP Pre-Process\n",
    "df1 = entier_func(directory = r\"D:\\2.Praxis( all Stuff)\\3. subject wise records\\3.Term 3\\1.CAPP\\Data_File\"\n",
    "            ,data = \"10_hotels_reviews.csv\"\n",
    "            ,column_drop = ['Rating','hotel_name']\n",
    "            ,col_name = 'Review'\n",
    "            ,hotel = 'Goa Woodlands Hotel')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Noun Lists: \n",
      "Showing the sample output:  ['toilet', 'service', 'reponse', 'time', 'thing', 'average', 'hmy', 'reminder', 'staff', 'equipment']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acces</th>\n",
       "      <th>accessibility</th>\n",
       "      <th>accessible</th>\n",
       "      <th>accommodation</th>\n",
       "      <th>action</th>\n",
       "      <th>actualniranjan</th>\n",
       "      <th>addition</th>\n",
       "      <th>adequate</th>\n",
       "      <th>adult</th>\n",
       "      <th>advait</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>worst</th>\n",
       "      <th>worth</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>youre</th>\n",
       "      <th>yummy</th>\n",
       "      <th>zomato</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>773 rows × 998 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     acces  accessibility  accessible  accommodation  action  actualniranjan  \\\n",
       "0        0              0           0              0       0               0   \n",
       "1        0              0           0              0       0               0   \n",
       "2        0              0           0              0       0               0   \n",
       "3        0              0           0              0       0               0   \n",
       "4        0              0           0              0       0               0   \n",
       "..     ...            ...         ...            ...     ...             ...   \n",
       "768      0              0           0              0       0               0   \n",
       "769      0              0           0              0       0               0   \n",
       "770      0              0           0              0       0               0   \n",
       "771      0              0           0              0       0               0   \n",
       "772      0              0           0              0       0               0   \n",
       "\n",
       "     addition  adequate  adult  advait  ...  work  working  world  worst  \\\n",
       "0           0         0      0       0  ...     0        0      0      0   \n",
       "1           0         0      0       0  ...     0        0      0      0   \n",
       "2           0         0      0       0  ...     0        0      0      0   \n",
       "3           0         0      0       0  ...     0        0      0      0   \n",
       "4           0         0      0       0  ...     1        0      0      0   \n",
       "..        ...       ...    ...     ...  ...   ...      ...    ...    ...   \n",
       "768         0         0      0       0  ...     0        0      0      0   \n",
       "769         0         0      0       0  ...     0        0      0      0   \n",
       "770         0         0      0       0  ...     0        0      0      0   \n",
       "771         0         0      0       0  ...     0        0      0      0   \n",
       "772         0         0      0       0  ...     0        0      0      0   \n",
       "\n",
       "     worth  year  yesterday  youre  yummy  zomato  \n",
       "0        0     0          0      0      0       0  \n",
       "1        0     0          0      0      0       0  \n",
       "2        0     0          0      0      0       0  \n",
       "3        0     0          0      0      0       0  \n",
       "4        0     0          0      0      0       0  \n",
       "..     ...   ...        ...    ...    ...     ...  \n",
       "768      0     0          0      0      0       0  \n",
       "769      0     0          0      0      0       0  \n",
       "770      0     0          0      0      0       0  \n",
       "771      0     0          0      0      0       0  \n",
       "772      0     0          0      0      0       0  \n",
       "\n",
       "[773 rows x 998 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary conversion\n",
    "data = binary_conversion(data='Review_Singular_Cnoun',colname = 'Review_Singular_Cnoun')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(bathroom)</td>\n",
       "      <td>(room)</td>\n",
       "      <td>0.040103</td>\n",
       "      <td>0.531695</td>\n",
       "      <td>0.032342</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>1.516757</td>\n",
       "      <td>0.011019</td>\n",
       "      <td>2.419577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(beach)</td>\n",
       "      <td>(hotel)</td>\n",
       "      <td>0.080207</td>\n",
       "      <td>0.548512</td>\n",
       "      <td>0.068564</td>\n",
       "      <td>0.854839</td>\n",
       "      <td>1.558468</td>\n",
       "      <td>0.024570</td>\n",
       "      <td>3.110249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(beach)</td>\n",
       "      <td>(location)</td>\n",
       "      <td>0.080207</td>\n",
       "      <td>0.275550</td>\n",
       "      <td>0.034929</td>\n",
       "      <td>0.435484</td>\n",
       "      <td>1.580418</td>\n",
       "      <td>0.012828</td>\n",
       "      <td>1.283312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(beach)</td>\n",
       "      <td>(room)</td>\n",
       "      <td>0.080207</td>\n",
       "      <td>0.531695</td>\n",
       "      <td>0.047865</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>1.122400</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>1.161397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(beach)</td>\n",
       "      <td>(staff)</td>\n",
       "      <td>0.080207</td>\n",
       "      <td>0.350582</td>\n",
       "      <td>0.033635</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>1.196167</td>\n",
       "      <td>0.005516</td>\n",
       "      <td>1.118442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>(value, money)</td>\n",
       "      <td>(room, staff)</td>\n",
       "      <td>0.099612</td>\n",
       "      <td>0.226391</td>\n",
       "      <td>0.031048</td>\n",
       "      <td>0.311688</td>\n",
       "      <td>1.376772</td>\n",
       "      <td>0.008497</td>\n",
       "      <td>1.123923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>(value, staff)</td>\n",
       "      <td>(room, money)</td>\n",
       "      <td>0.056921</td>\n",
       "      <td>0.065977</td>\n",
       "      <td>0.031048</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>8.267380</td>\n",
       "      <td>0.027292</td>\n",
       "      <td>2.054851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>(money, staff)</td>\n",
       "      <td>(room, value)</td>\n",
       "      <td>0.062096</td>\n",
       "      <td>0.059508</td>\n",
       "      <td>0.031048</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.402174</td>\n",
       "      <td>0.027353</td>\n",
       "      <td>1.880983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>(value)</td>\n",
       "      <td>(room, money, staff)</td>\n",
       "      <td>0.108668</td>\n",
       "      <td>0.034929</td>\n",
       "      <td>0.031048</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>8.179894</td>\n",
       "      <td>0.027252</td>\n",
       "      <td>1.351100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>(money)</td>\n",
       "      <td>(room, value, staff)</td>\n",
       "      <td>0.126779</td>\n",
       "      <td>0.034929</td>\n",
       "      <td>0.031048</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>7.011338</td>\n",
       "      <td>0.026620</td>\n",
       "      <td>1.278067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>817 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        antecedents           consequents  antecedent support  \\\n",
       "0        (bathroom)                (room)            0.040103   \n",
       "1           (beach)               (hotel)            0.080207   \n",
       "2           (beach)            (location)            0.080207   \n",
       "3           (beach)                (room)            0.080207   \n",
       "4           (beach)               (staff)            0.080207   \n",
       "..              ...                   ...                 ...   \n",
       "812  (value, money)         (room, staff)            0.099612   \n",
       "813  (value, staff)         (room, money)            0.056921   \n",
       "814  (money, staff)         (room, value)            0.062096   \n",
       "815         (value)  (room, money, staff)            0.108668   \n",
       "816         (money)  (room, value, staff)            0.126779   \n",
       "\n",
       "     consequent support   support  confidence      lift  leverage  conviction  \n",
       "0              0.531695  0.032342    0.806452  1.516757  0.011019    2.419577  \n",
       "1              0.548512  0.068564    0.854839  1.558468  0.024570    3.110249  \n",
       "2              0.275550  0.034929    0.435484  1.580418  0.012828    1.283312  \n",
       "3              0.531695  0.047865    0.596774  1.122400  0.005220    1.161397  \n",
       "4              0.350582  0.033635    0.419355  1.196167  0.005516    1.118442  \n",
       "..                  ...       ...         ...       ...       ...         ...  \n",
       "812            0.226391  0.031048    0.311688  1.376772  0.008497    1.123923  \n",
       "813            0.065977  0.031048    0.545455  8.267380  0.027292    2.054851  \n",
       "814            0.059508  0.031048    0.500000  8.402174  0.027353    1.880983  \n",
       "815            0.034929  0.031048    0.285714  8.179894  0.027252    1.351100  \n",
       "816            0.034929  0.031048    0.244898  7.011338  0.026620    1.278067  \n",
       "\n",
       "[817 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Association Rule\n",
    "rules = association_rule(data,dup_col1='antecedents',dup_col2='consequents')\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Max Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(bathroom)</td>\n",
       "      <td>(room)</td>\n",
       "      <td>0.032342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(beach)</td>\n",
       "      <td>(hotel)</td>\n",
       "      <td>0.068564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(beach)</td>\n",
       "      <td>(location)</td>\n",
       "      <td>0.034929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(beach)</td>\n",
       "      <td>(room)</td>\n",
       "      <td>0.047865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(beach)</td>\n",
       "      <td>(staff)</td>\n",
       "      <td>0.033635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>(service, staff)</td>\n",
       "      <td>(stay, hotel)</td>\n",
       "      <td>0.032342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>(room, money, staff)</td>\n",
       "      <td>(value)</td>\n",
       "      <td>0.031048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>(room, money)</td>\n",
       "      <td>(value, staff)</td>\n",
       "      <td>0.031048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>(money, staff)</td>\n",
       "      <td>(room, value)</td>\n",
       "      <td>0.031048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>(money)</td>\n",
       "      <td>(room, value, staff)</td>\n",
       "      <td>0.031048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>672 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              antecedents           consequents   support\n",
       "0              (bathroom)                (room)  0.032342\n",
       "1                 (beach)               (hotel)  0.068564\n",
       "2                 (beach)            (location)  0.034929\n",
       "3                 (beach)                (room)  0.047865\n",
       "4                 (beach)               (staff)  0.033635\n",
       "..                    ...                   ...       ...\n",
       "667      (service, staff)         (stay, hotel)  0.032342\n",
       "668  (room, money, staff)               (value)  0.031048\n",
       "669         (room, money)        (value, staff)  0.031048\n",
       "670        (money, staff)         (room, value)  0.031048\n",
       "671               (money)  (room, value, staff)  0.031048\n",
       "\n",
       "[672 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the Max Support and removing the duplicates\n",
    "new_data = max_support_finding(rules,one_of_asso_column='support')\n",
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compactness Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['beach', 'hotel'],\n",
       " ['beach', 'location'],\n",
       " ['money', 'breakfast'],\n",
       " ['value', 'breakfast'],\n",
       " ['city', 'hotel'],\n",
       " ['everything', 'room'],\n",
       " ['experience', 'food'],\n",
       " ['experience', 'hotel'],\n",
       " ['experience', 'location'],\n",
       " ['experience', 'room'],\n",
       " ['experience', 'service'],\n",
       " ['experience', 'staff'],\n",
       " ['facility', 'food'],\n",
       " ['facility', 'hotel'],\n",
       " ['facility', 'location'],\n",
       " ['facility', 'staff'],\n",
       " ['family', 'hotel'],\n",
       " ['family', 'staff'],\n",
       " ['family', 'stay'],\n",
       " ['market', 'food'],\n",
       " ['money', 'food'],\n",
       " ['place', 'food'],\n",
       " ['quality', 'food'],\n",
       " ['railway', 'food'],\n",
       " ['restaurant', 'food'],\n",
       " ['time', 'food'],\n",
       " ['value', 'food'],\n",
       " ['market', 'hotel'],\n",
       " ['money', 'hotel'],\n",
       " ['place', 'hotel'],\n",
       " ['restaurant', 'hotel'],\n",
       " ['station', 'hotel'],\n",
       " ['time', 'hotel'],\n",
       " ['trip', 'hotel'],\n",
       " ['value', 'hotel'],\n",
       " ['woodland', 'hotel'],\n",
       " ['money', 'location'],\n",
       " ['value', 'location'],\n",
       " ['market', 'service'],\n",
       " ['market', 'staff'],\n",
       " ['money', 'room'],\n",
       " ['money', 'service'],\n",
       " ['money', 'stay'],\n",
       " ['place', 'room'],\n",
       " ['place', 'stay'],\n",
       " ['quality', 'room'],\n",
       " ['quality', 'service'],\n",
       " ['quality', 'staff'],\n",
       " ['railway', 'room'],\n",
       " ['railway', 'service'],\n",
       " ['restaurant', 'service'],\n",
       " ['restaurant', 'staff'],\n",
       " ['station', 'room'],\n",
       " ['time', 'room'],\n",
       " ['value', 'room'],\n",
       " ['station', 'service'],\n",
       " ['time', 'service'],\n",
       " ['value', 'service'],\n",
       " ['food', 'breakfast'],\n",
       " ['hotel', 'breakfast'],\n",
       " ['location', 'breakfast'],\n",
       " ['breakfast', 'room'],\n",
       " ['service', 'breakfast'],\n",
       " ['staff', 'breakfast'],\n",
       " ['stay', 'breakfast'],\n",
       " ['hotel', 'food'],\n",
       " ['location', 'food'],\n",
       " ['food', 'room'],\n",
       " ['service', 'food'],\n",
       " ['staff', 'food'],\n",
       " ['food', 'stay'],\n",
       " ['location', 'hotel'],\n",
       " ['hotel', 'room'],\n",
       " ['service', 'hotel'],\n",
       " ['staff', 'hotel'],\n",
       " ['hotel', 'stay'],\n",
       " ['location', 'room'],\n",
       " ['location', 'service'],\n",
       " ['staff', 'location'],\n",
       " ['location', 'stay'],\n",
       " ['service', 'room'],\n",
       " ['staff', 'room'],\n",
       " ['stay', 'room'],\n",
       " ['staff', 'service'],\n",
       " ['service', 'stay'],\n",
       " ['staff', 'stay']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compactness Pruning\n",
    "comp = compact_pruning()\n",
    "comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redundancy Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('beach', 'hotel'): 9,\n",
       " ('beach', 'location'): 2,\n",
       " ('money', 'breakfast'): 2,\n",
       " ('value', 'breakfast'): 2,\n",
       " ('city', 'hotel'): 6,\n",
       " ('everything', 'room'): 4,\n",
       " ('experience', 'food'): 5,\n",
       " ('experience', 'hotel'): 7,\n",
       " ('experience', 'location'): 2,\n",
       " ('experience', 'room'): 3,\n",
       " ('experience', 'service'): 2,\n",
       " ('experience', 'staff'): 3,\n",
       " ('facility', 'food'): 2,\n",
       " ('facility', 'hotel'): 3,\n",
       " ('facility', 'location'): 2,\n",
       " ('facility', 'staff'): 2,\n",
       " ('family', 'hotel'): 3,\n",
       " ('family', 'staff'): 2,\n",
       " ('family', 'stay'): 4,\n",
       " ('market', 'food'): 4,\n",
       " ('money', 'food'): 3,\n",
       " ('place', 'food'): 3,\n",
       " ('quality', 'food'): 3,\n",
       " ('railway', 'food'): 2,\n",
       " ('restaurant', 'food'): 17,\n",
       " ('time', 'food'): 2,\n",
       " ('value', 'food'): 3,\n",
       " ('market', 'hotel'): 5,\n",
       " ('money', 'hotel'): 7,\n",
       " ('place', 'hotel'): 5,\n",
       " ('restaurant', 'hotel'): 5,\n",
       " ('station', 'hotel'): 4,\n",
       " ('time', 'hotel'): 7,\n",
       " ('trip', 'hotel'): 5,\n",
       " ('value', 'hotel'): 8,\n",
       " ('woodland', 'hotel'): 9,\n",
       " ('money', 'location'): 3,\n",
       " ('value', 'location'): 3,\n",
       " ('market', 'service'): 3,\n",
       " ('market', 'staff'): 3,\n",
       " ('money', 'room'): 2,\n",
       " ('money', 'service'): 2,\n",
       " ('money', 'stay'): 5,\n",
       " ('place', 'room'): 2,\n",
       " ('place', 'stay'): 9,\n",
       " ('quality', 'room'): 3,\n",
       " ('quality', 'service'): 4,\n",
       " ('quality', 'staff'): 2,\n",
       " ('railway', 'room'): 2,\n",
       " ('railway', 'service'): 4,\n",
       " ('restaurant', 'service'): 5,\n",
       " ('restaurant', 'staff'): 6,\n",
       " ('station', 'room'): 2,\n",
       " ('time', 'room'): 4,\n",
       " ('value', 'room'): 4,\n",
       " ('station', 'service'): 5,\n",
       " ('time', 'service'): 3,\n",
       " ('value', 'service'): 2,\n",
       " ('food', 'breakfast'): 18,\n",
       " ('hotel', 'breakfast'): 7,\n",
       " ('location', 'breakfast'): 6,\n",
       " ('breakfast', 'room'): 4,\n",
       " ('service', 'breakfast'): 8,\n",
       " ('staff', 'breakfast'): 5,\n",
       " ('stay', 'breakfast'): 8,\n",
       " ('hotel', 'food'): 29,\n",
       " ('location', 'food'): 14,\n",
       " ('food', 'room'): 19,\n",
       " ('service', 'food'): 23,\n",
       " ('staff', 'food'): 13,\n",
       " ('food', 'stay'): 3,\n",
       " ('location', 'hotel'): 26,\n",
       " ('hotel', 'room'): 29,\n",
       " ('service', 'hotel'): 18,\n",
       " ('staff', 'hotel'): 13,\n",
       " ('hotel', 'stay'): 48,\n",
       " ('location', 'room'): 12,\n",
       " ('location', 'service'): 15,\n",
       " ('staff', 'location'): 4,\n",
       " ('location', 'stay'): 3,\n",
       " ('service', 'room'): 11,\n",
       " ('staff', 'room'): 11,\n",
       " ('stay', 'room'): 11,\n",
       " ('staff', 'service'): 18,\n",
       " ('service', 'stay'): 7,\n",
       " ('staff', 'stay'): 4}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two Feature Counting\n",
    "twofeat = twofeats()\n",
    "twofeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beach': 37,\n",
       " 'breakfast': 194,\n",
       " 'city': 57,\n",
       " 'everything': 45,\n",
       " 'experience': 63,\n",
       " 'facility': 25,\n",
       " 'family': 52,\n",
       " 'food': 279,\n",
       " 'hotel': 582,\n",
       " 'location': 199,\n",
       " 'market': 60,\n",
       " 'money': 47,\n",
       " 'place': 65,\n",
       " 'quality': 58,\n",
       " 'railway': 69,\n",
       " 'restaurant': 75,\n",
       " 'room': 301,\n",
       " 'service': 251,\n",
       " 'staff': 244,\n",
       " 'station': 51,\n",
       " 'stay': 269,\n",
       " 'time': 43,\n",
       " 'trip': 27,\n",
       " 'value': 82,\n",
       " 'woodland': 37}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single Feature Counting\n",
    "sinfeat = single_feat()\n",
    "sinfeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beach': 26,\n",
       " 'breakfast': 134,\n",
       " 'city': 51,\n",
       " 'everything': 41,\n",
       " 'experience': 41,\n",
       " 'facility': 16,\n",
       " 'family': 43,\n",
       " 'food': 116,\n",
       " 'hotel': 329,\n",
       " 'location': 107,\n",
       " 'market': 45,\n",
       " 'money': 23,\n",
       " 'place': 46,\n",
       " 'quality': 46,\n",
       " 'railway': 61,\n",
       " 'restaurant': 42,\n",
       " 'room': 178,\n",
       " 'service': 121,\n",
       " 'staff': 158,\n",
       " 'station': 40,\n",
       " 'stay': 167,\n",
       " 'time': 27,\n",
       " 'trip': 22,\n",
       " 'value': 60,\n",
       " 'woodland': 28}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the P-Support form (Single Feature - sum(Two Feature))\n",
    "p_support(twofeat,sinfeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
