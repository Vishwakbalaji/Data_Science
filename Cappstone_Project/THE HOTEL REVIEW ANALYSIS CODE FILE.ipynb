{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hotel Review Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\vishw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vishw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "string.punctuation\n",
    "import re\n",
    "from textblob import TextBlob # using text blob function\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "import gensim\n",
    "from pattern.text.en import singularize\n",
    "punctuations = '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~'\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "#NLP Pre-Process:\n",
    "def entier_func(directory,data,column_drop,col_name,hotel=0):\n",
    "    os.chdir(directory)\n",
    "    os.getcwd()\n",
    "    \n",
    "    df = pd.read_csv(data)\n",
    "    df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "    df.dropna(axis=0,inplace=True)\n",
    "    if hotel == 0:\n",
    "        df1 = df.drop(column_drop,axis=1)\n",
    "    else:\n",
    "        df1 = df[df['hotel_name'] == hotel]\n",
    "        df1 = df1.drop(column_drop,axis=1)\n",
    "    # pre-processing:\n",
    "    \n",
    "    # punctuation removal\n",
    "    def remove_punctuation(text):\n",
    "        punctuation_free=\"\".join([i for i in text if i not in punctuations])\n",
    "        return punctuation_free\n",
    "    \n",
    "    df1['Review_no_punc']=df1[col_name].apply(lambda x: remove_punctuation(x))\n",
    "    \n",
    "    # lowering text\n",
    "    df1['Review_lower']= df1['Review_no_punc'].apply(lambda x: x.lower())\n",
    "    \n",
    "    # extra white space removal\n",
    "    df1['Review_no_extra_space'] = df1['Review_lower'].apply(lambda text: re.sub(' +', ' ', text))\n",
    "    \n",
    "    # emoji removal\n",
    "    def emoji(string):  # created a emoji removing funcction\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', string)\n",
    "    \n",
    "    df1['Review_no_emoji'] = df1['Review_no_extra_space'].apply(emoji)\n",
    "    \n",
    "    ## ------------- For compactness pruning and redundancy pruning----------------\n",
    "    # Sentence Tokenization:\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    df1['Review_sent_token'] = df1['Review_no_emoji'].apply(sent_tokenize)\n",
    "        \n",
    "    # stop words removal:\n",
    "    def stop_words_removal(text):\n",
    "        result = []\n",
    "        for token in gensim.utils.simple_preprocess(text):\n",
    "            if token not in stop_words:\n",
    "                result.append(token)\n",
    "        return result\n",
    "    \n",
    "    b = df1['Review_no_emoji'].apply(stop_words_removal)\n",
    "    df1['Stop_words_removal'] = b.apply(lambda x: \" \".join(x))\n",
    "    ## -----------------------------------------------------------------------------\n",
    "    \n",
    "    # word Tokenization:\n",
    "    def tokenization(text):\n",
    "        tokens = re.split('W+',text)\n",
    "        return tokens\n",
    "    \n",
    "    a = df1['Review_no_emoji'].apply(lambda x: tokenization(x)) # calling the function\n",
    "    df1['Review_token'] = a.apply(lambda x: \" \".join(x)) # Join the words into the String\n",
    "    \n",
    "    # remove the words less than two words:\n",
    "    def greater_than_2(text):\n",
    "        result = []\n",
    "        for token in gensim.utils.simple_preprocess(text):\n",
    "            if len(token) > 3:\n",
    "                result.append(token)\n",
    "        return result\n",
    "    \n",
    "    b = df1['Review_token'].apply(lambda x: greater_than_2(x)) \n",
    "    df1['Review_above_3token'] = b.apply(lambda x: \" \".join(x)) # Join the words into the String\n",
    "    # Necessary Tagging:\n",
    "    \n",
    "    # parts of speech tagging\n",
    "    def pos_tag(text):\n",
    "        try:\n",
    "            return TextBlob(text).tags\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    df1['Review_pos_tag'] = df1['Review_above_3token'].apply((pos_tag)) # calling pos_tag function \n",
    "    \n",
    "    # addjective tagging\n",
    "    def get_adjectives(text):\n",
    "        blob = TextBlob(text)\n",
    "        return [ word for (word,tag) in blob.tags if tag == \"JJ\"]\n",
    "    \n",
    "    df1['Review_adjective'] = df1['Review_above_3token'].apply(get_adjectives)\n",
    "    \n",
    "    # Common Noun(NN and NNS) [which is called Noun and as well as common noun and NNP, NNPS is called proper Noun]\n",
    "    def get_common_nouns(text):\n",
    "        blob = TextBlob(text)\n",
    "        return [ word for (word,tag) in blob.tags if tag == \"NN\" or tag == \"NNS\"]\n",
    "    \n",
    "    df1['Review_Common_Noun'] = df1['Review_above_3token'].apply(get_common_nouns)\n",
    "    df1.reset_index(inplace = True, drop = True) # changed the index structure\n",
    "    \n",
    "    # converting it into singular\n",
    "    def singularity(texts):\n",
    "        return [singularize(text) for text in texts]\n",
    "    # calling the function and saving it as like singular noun\n",
    "    df1['Review_Singular_Cnoun'] = df1['Review_Common_Noun'].apply(lambda x: singularity(x)) # calling the function\n",
    "    print(df1.shape)\n",
    "    return df1\n",
    "\n",
    "# Binary Conversion:\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "def binary_conversion(data,colname):\n",
    "    mlb = MultiLabelBinarizer() # binary converter\n",
    "    data = pd.DataFrame(mlb.fit_transform(df1[data]),columns=mlb.classes_)\n",
    "    \n",
    "    # getting only unique noun list(which is used in binary columns)\n",
    "    unique_noun_list = []   # it is for unique words\n",
    "    for i in range(0,len(df1[colname])): # iterating the len of noun column\n",
    "        for j in df1[colname][i]:        # iterating the text of each index in noun column\n",
    "            if j not in unique_noun_list:      # if the iterated text not presented inside the list\n",
    "                unique_noun_list.append(j)     # then we append it into the list\n",
    "            else:                              # if not we leave it and continue\n",
    "                continue\n",
    "    print(\"Unique Noun Lists: \")\n",
    "    print(\"Showing the sample output: \",unique_noun_list[0:10])\n",
    "    return data\n",
    "\n",
    "# Association Rule\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "def association_rule(data,dup_col1,dup_col2):\n",
    "    df2 = data.copy()\n",
    "    #using Apriori rule to find the frequent set\n",
    "    freq_reviewset = apriori(df = df2, min_support = 0.03, use_colnames=True) # fixing the support as 0.03\n",
    "    freq_reviewset.to_csv('frequent_review_set.csv', encoding='utf-8')\n",
    "    \n",
    "    rules = association_rules(freq_reviewset,min_threshold=0.2) #fixing threshold as 0.2\n",
    "    rules.to_csv('association_rule.csv', encoding='utf-8')\n",
    "    \n",
    "    rules.drop_duplicates(subset=[dup_col1,dup_col2],keep='first',inplace=True)\n",
    "    return rules\n",
    "\n",
    "\n",
    "# Finding the max support and removing the duplicates\n",
    "\n",
    "def max_support_finding(rules,one_of_asso_column):\n",
    "    rules1 = rules[['antecedents','consequents','support']]\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    max = []\n",
    "    \n",
    "    df2 = rules1\n",
    "    dfs =  pd.DataFrame()\n",
    "    dbs =  pd.DataFrame()\n",
    "    \n",
    "    for x in range(len(df2)):\n",
    "        l1.append(df2.iloc[x,0])\n",
    "        l2.append(df2.iloc[x,1])\n",
    "    \n",
    "    for x in range(len(l1)):\n",
    "        for y in range(len(df2)):\n",
    "            if (l1[x] == df2.iloc[y,1]) and (l2[x] == df2.iloc[y,0]):\n",
    "                kd = (df2.iloc[[x],:])\n",
    "                dfs = pd.concat([dfs,kd],axis = 0)\n",
    "                dfs.sort_values(by =one_of_asso_column, ascending = False)\n",
    "    \n",
    "    f = list(dfs.index)\n",
    "\n",
    "    dfnon = df2.drop(f)\n",
    "    \n",
    "\n",
    "    for x in range(len(dfs)):\n",
    "        if x % 2 == 0:\n",
    "            s = dfs.iloc[x:x+2 , 0:2].max()\n",
    "            max.append(dfs.iloc[x:x+2,2].max())\n",
    "            #print(s)\n",
    "            dbs = pd.concat([dbs,s],axis = 1)\n",
    "    \n",
    "    dbs = dbs.T.reset_index(drop=True)\n",
    "    \n",
    "    dbs[one_of_asso_column] = max\n",
    "\n",
    "    new_data = pd.concat([dfnon , dbs])\n",
    "\n",
    "    new_data.reset_index(drop=True,inplace = True)\n",
    "\n",
    "    return (new_data)\n",
    "\n",
    "\n",
    "#Compactness Pruning\n",
    "def compact_pruning():\n",
    "    prun_data = new_data.copy()\n",
    "\n",
    "    # converting the frozenset to list\n",
    "    prun_data['antecedents'] = prun_data[['antecedents']].applymap(lambda x: list(x))\n",
    "    prun_data['consequents'] = prun_data[['consequents']].applymap(lambda x: list(x))\n",
    "\n",
    "    # conversion from list to str\n",
    "    prun_data['antecedents'] = prun_data[['antecedents']].applymap(lambda x: \" \".join(x))\n",
    "    prun_data['consequents'] = prun_data[['consequents']].applymap(lambda x: \" \".join(x))\n",
    "\n",
    "    comp = []\n",
    "    first = []\n",
    "    second = []\n",
    "    count=0\n",
    "    k=0\n",
    "    mindis = []\n",
    "    count = 0\n",
    "    feat = []\n",
    "    w = []\n",
    "\n",
    "    s = []\n",
    "    for i in df1['Review_sent_token']:\n",
    "        for j in i:\n",
    "            s.append(j.split( ))\n",
    "\n",
    "    for i,j in zip(prun_data['antecedents'],prun_data['consequents']):\n",
    "        feat.append(i)\n",
    "        feat.append(j)\n",
    "        #print(feat)\n",
    "        for i in s:\n",
    "            for word in i: \n",
    "                if word not in stop_words:\n",
    "                    w.append(word)\n",
    "            #print(w)\n",
    "            if (feat[0] in w and feat[1] in w):\n",
    "                #print(w)\n",
    "                for j in w:\n",
    "                    #print(j)\n",
    "                    if j == feat[0]:\n",
    "                        #print(j)\n",
    "                        k =w.index(feat[0],k)\n",
    "                        #print(k)\n",
    "                        first.append(k)\n",
    "                        k += 1\n",
    "                k = 0\n",
    "                for j in w:\n",
    "                    #print(j)\n",
    "                    if(j==feat[1]):\n",
    "                        k = w.index(feat[1],k)\n",
    "                        second.append(k)\n",
    "                        k += 1\n",
    "                for i in first:\n",
    "                    for j in second:\n",
    "                        if(i<j):\n",
    "                            mindis.append((j-i)-1)\n",
    "                    #print(mindis)\n",
    "                    if(mindis == []):\n",
    "                        break\n",
    "                    if (min(mindis)) <= 5:\n",
    "                        #print(min(mindis))\n",
    "                        count+= 1\n",
    "                    #print(count)\n",
    "                k = 0\n",
    "            w = []\n",
    "            first = []\n",
    "            second = []\n",
    "            mindis = []\n",
    "        if (count >= 2):\n",
    "                comp.append(feat)\n",
    "        feat = []\n",
    "        count = 0\n",
    "    df_comp = pd.DataFrame(comp)\n",
    "    df_comp = df_comp.rename(columns = {0:'antecedents',1:'consequents'}, inplace = False)\n",
    "    \n",
    "    return (comp)\n",
    "\n",
    "# Redundancy Pruning\n",
    "\n",
    "# 1.two feature feature for redundancy pruning:\n",
    "def twofeats():\n",
    "    twofeat = {}\n",
    "    w = []\n",
    "    feat = []\n",
    "    k = 0\n",
    "    count = 0\n",
    "    s = []\n",
    "    for i in df1['Review_sent_token']:\n",
    "        for j in i:\n",
    "            s.append(j.split( ))\n",
    "    for i,j in comp:\n",
    "        feat.append(i)\n",
    "        feat.append(j)\n",
    "        #print(feat)\n",
    "        for i in s:\n",
    "            for word in i: \n",
    "                if word not in stop_words:\n",
    "                    w.append(word)\n",
    "            #print(w)\n",
    "            if (feat[0] in w and feat[1] in w):\n",
    "                #print(w)\n",
    "                for j in w:\n",
    "                    #print(j)\n",
    "                    if j == feat[0]:\n",
    "                        #print(j)\n",
    "                        k =w.index(feat[0],k)\n",
    "                        #print(k)\n",
    "                        first.append(k)\n",
    "                        k += 1\n",
    "                k = 0\n",
    "                for j in w:\n",
    "                    #print(j)\n",
    "                    if(j==feat[1]):\n",
    "                        k = w.index(feat[1],k)\n",
    "                        second.append(k)\n",
    "                        k += 1\n",
    "                for i in first:\n",
    "                    for j in second:\n",
    "                        if(i<j):\n",
    "                            mindis.append((j-i)-1)\n",
    "                    #print(mindis)\n",
    "                    if(mindis == []):\n",
    "                        break\n",
    "                    if (min(mindis)) <= 5:\n",
    "                        #print(min(mindis))\n",
    "                        count+= 1\n",
    "                    #print(count)\n",
    "                k = 0\n",
    "            w = []\n",
    "            first = []\n",
    "            second = []\n",
    "            mindis = []\n",
    "        #print(feat)\n",
    "        twofeat[feat[0],feat[1]] = count\n",
    "        feat = []\n",
    "        #print(count)\n",
    "        count = 0\n",
    "    return twofeat\n",
    "\n",
    "# 2. single feature for redundancy pruning:\n",
    "def single_feat():\n",
    "    count = 0\n",
    "    sinfeat = {}\n",
    "    s = []\n",
    "    for i in df1['Review_sent_token']:\n",
    "        for j in i:\n",
    "            s.append(j.split( ))\n",
    "    for l in np.unique(comp):\n",
    "        for i in s:\n",
    "            for j in i:\n",
    "                if(j == l):\n",
    "                    count+= 1\n",
    "        #print(l)\n",
    "        #print(count)\n",
    "        sinfeat[l] = count\n",
    "        count = 0\n",
    "    return sinfeat\n",
    "# 3. p support redundancy pruning:\n",
    "def p_support(twofeat,sinfeat):\n",
    "    sum2 = 0\n",
    "    support = 0\n",
    "    red = {}\n",
    "    for j in sinfeat:\n",
    "        #print(j)\n",
    "        for i in twofeat:\n",
    "            #print(i)\n",
    "            if(j in i):\n",
    "                sum2 += twofeat[i]\n",
    "        support = sinfeat[j] - sum2\n",
    "        red[j] = support\n",
    "        support = 0\n",
    "        sum2 = 0\n",
    "    return red\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Pre-Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 Hotel Names:\n",
    "- Ocean Palms Goa\n",
    "- Silver Sands Serenity\n",
    "- Goa Woodlands Hotel\n",
    "- The Byke Old Anchor Beach Resort & Spa\n",
    "- Whispering Palms Beach Resort\n",
    "- Rivasa Resort\n",
    "- Rendezvous Beach Resort\n",
    "- Hotel Campal\n",
    "- Hotel MR Manfred\n",
    "- Hotel Royal Palace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6046, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_no_punc</th>\n",
       "      <th>Review_lower</th>\n",
       "      <th>Review_no_extra_space</th>\n",
       "      <th>Review_no_emoji</th>\n",
       "      <th>Review_sent_token</th>\n",
       "      <th>Stop_words_removal</th>\n",
       "      <th>Review_token</th>\n",
       "      <th>Review_above_3token</th>\n",
       "      <th>Review_pos_tag</th>\n",
       "      <th>Review_adjective</th>\n",
       "      <th>Review_Common_Noun</th>\n",
       "      <th>Review_Singular_Cnoun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good hotel, great staffs, nice food and servic...</td>\n",
       "      <td>Good hotel great staffs nice food and service....</td>\n",
       "      <td>good hotel great staffs nice food and service....</td>\n",
       "      <td>good hotel great staffs nice food and service....</td>\n",
       "      <td>good hotel great staffs nice food and service....</td>\n",
       "      <td>[good hotel great staffs nice food and service...</td>\n",
       "      <td>good hotel great staffs nice food service near...</td>\n",
       "      <td>good hotel great staffs nice food and service....</td>\n",
       "      <td>good hotel great staffs nice food service near...</td>\n",
       "      <td>[(good, JJ), (hotel, NN), (great, JJ), (staffs...</td>\n",
       "      <td>[good, great, nice]</td>\n",
       "      <td>[hotel, staffs, food, service, beach, shopping...</td>\n",
       "      <td>[hotel, staff, food, service, beach, shopping,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Very good hotel, nice service we will plan nex...</td>\n",
       "      <td>Very good hotel nice service we will plan next...</td>\n",
       "      <td>very good hotel nice service we will plan next...</td>\n",
       "      <td>very good hotel nice service we will plan next...</td>\n",
       "      <td>very good hotel nice service we will plan next...</td>\n",
       "      <td>[very good hotel nice service we will plan nex...</td>\n",
       "      <td>good hotel nice service plan next vacation als...</td>\n",
       "      <td>very good hotel nice service we will plan next...</td>\n",
       "      <td>very good hotel nice service will plan next va...</td>\n",
       "      <td>[(very, RB), (good, JJ), (hotel, NN), (nice, J...</td>\n",
       "      <td>[good, nice, next, friendly]</td>\n",
       "      <td>[hotel, service, vacation, family, friends, st...</td>\n",
       "      <td>[hotel, service, vacation, family, friend, sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Far away from the over crowded Calangute and B...</td>\n",
       "      <td>Far away from the over crowded Calangute and B...</td>\n",
       "      <td>far away from the over crowded calangute and b...</td>\n",
       "      <td>far away from the over crowded calangute and b...</td>\n",
       "      <td>far away from the over crowded calangute and b...</td>\n",
       "      <td>[far away from the over crowded calangute and ...</td>\n",
       "      <td>far away crowded calangute baga whispering pal...</td>\n",
       "      <td>far away from the over crowded calangute and b...</td>\n",
       "      <td>away from over crowded calangute baga whisperi...</td>\n",
       "      <td>[(away, RB), (from, IN), (over, IN), (crowded,...</td>\n",
       "      <td>[crowded, candolim, perfect, commercial, allot...</td>\n",
       "      <td>[calangute, baga, palms, location, access, gat...</td>\n",
       "      <td>[calangute, baga, palm, location, acces, gate,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very close to Candolim beach. Nice food and bu...</td>\n",
       "      <td>Very close to Candolim beach. Nice food and bu...</td>\n",
       "      <td>very close to candolim beach. nice food and bu...</td>\n",
       "      <td>very close to candolim beach. nice food and bu...</td>\n",
       "      <td>very close to candolim beach. nice food and bu...</td>\n",
       "      <td>[very close to candolim beach., nice food and ...</td>\n",
       "      <td>close candolim beach nice food buffet swimming...</td>\n",
       "      <td>very close to candolim beach. nice food and bu...</td>\n",
       "      <td>very close candolim beach nice food buffet swi...</td>\n",
       "      <td>[(very, RB), (close, RB), (candolim, JJ), (bea...</td>\n",
       "      <td>[candolim, nice, good]</td>\n",
       "      <td>[beach, food, buffet, pool]</td>\n",
       "      <td>[beach, food, buffet, pool]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Very nice ambience, just 2 mins walk from reso...</td>\n",
       "      <td>Very nice ambience just 2 mins walk from resor...</td>\n",
       "      <td>very nice ambience just 2 mins walk from resor...</td>\n",
       "      <td>very nice ambience just 2 mins walk from resor...</td>\n",
       "      <td>very nice ambience just 2 mins walk from resor...</td>\n",
       "      <td>[very nice ambience just 2 mins walk from reso...</td>\n",
       "      <td>nice ambience mins walk resort beach though ro...</td>\n",
       "      <td>very nice ambience just 2 mins walk from resor...</td>\n",
       "      <td>very nice ambience just mins walk from resort ...</td>\n",
       "      <td>[(very, RB), (nice, JJ), (ambience, NN), (just...</td>\n",
       "      <td>[nice, good, good]</td>\n",
       "      <td>[ambience, mins, resort, beach, rooms, food, p...</td>\n",
       "      <td>[ambience, min, resort, beach, room, food, pla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  \\\n",
       "0  Good hotel, great staffs, nice food and servic...   \n",
       "1  Very good hotel, nice service we will plan nex...   \n",
       "2  Far away from the over crowded Calangute and B...   \n",
       "3  Very close to Candolim beach. Nice food and bu...   \n",
       "4  Very nice ambience, just 2 mins walk from reso...   \n",
       "\n",
       "                                      Review_no_punc  \\\n",
       "0  Good hotel great staffs nice food and service....   \n",
       "1  Very good hotel nice service we will plan next...   \n",
       "2  Far away from the over crowded Calangute and B...   \n",
       "3  Very close to Candolim beach. Nice food and bu...   \n",
       "4  Very nice ambience just 2 mins walk from resor...   \n",
       "\n",
       "                                        Review_lower  \\\n",
       "0  good hotel great staffs nice food and service....   \n",
       "1  very good hotel nice service we will plan next...   \n",
       "2  far away from the over crowded calangute and b...   \n",
       "3  very close to candolim beach. nice food and bu...   \n",
       "4  very nice ambience just 2 mins walk from resor...   \n",
       "\n",
       "                               Review_no_extra_space  \\\n",
       "0  good hotel great staffs nice food and service....   \n",
       "1  very good hotel nice service we will plan next...   \n",
       "2  far away from the over crowded calangute and b...   \n",
       "3  very close to candolim beach. nice food and bu...   \n",
       "4  very nice ambience just 2 mins walk from resor...   \n",
       "\n",
       "                                     Review_no_emoji  \\\n",
       "0  good hotel great staffs nice food and service....   \n",
       "1  very good hotel nice service we will plan next...   \n",
       "2  far away from the over crowded calangute and b...   \n",
       "3  very close to candolim beach. nice food and bu...   \n",
       "4  very nice ambience just 2 mins walk from resor...   \n",
       "\n",
       "                                   Review_sent_token  \\\n",
       "0  [good hotel great staffs nice food and service...   \n",
       "1  [very good hotel nice service we will plan nex...   \n",
       "2  [far away from the over crowded calangute and ...   \n",
       "3  [very close to candolim beach., nice food and ...   \n",
       "4  [very nice ambience just 2 mins walk from reso...   \n",
       "\n",
       "                                  Stop_words_removal  \\\n",
       "0  good hotel great staffs nice food service near...   \n",
       "1  good hotel nice service plan next vacation als...   \n",
       "2  far away crowded calangute baga whispering pal...   \n",
       "3  close candolim beach nice food buffet swimming...   \n",
       "4  nice ambience mins walk resort beach though ro...   \n",
       "\n",
       "                                        Review_token  \\\n",
       "0  good hotel great staffs nice food and service....   \n",
       "1  very good hotel nice service we will plan next...   \n",
       "2  far away from the over crowded calangute and b...   \n",
       "3  very close to candolim beach. nice food and bu...   \n",
       "4  very nice ambience just 2 mins walk from resor...   \n",
       "\n",
       "                                 Review_above_3token  \\\n",
       "0  good hotel great staffs nice food service near...   \n",
       "1  very good hotel nice service will plan next va...   \n",
       "2  away from over crowded calangute baga whisperi...   \n",
       "3  very close candolim beach nice food buffet swi...   \n",
       "4  very nice ambience just mins walk from resort ...   \n",
       "\n",
       "                                      Review_pos_tag  \\\n",
       "0  [(good, JJ), (hotel, NN), (great, JJ), (staffs...   \n",
       "1  [(very, RB), (good, JJ), (hotel, NN), (nice, J...   \n",
       "2  [(away, RB), (from, IN), (over, IN), (crowded,...   \n",
       "3  [(very, RB), (close, RB), (candolim, JJ), (bea...   \n",
       "4  [(very, RB), (nice, JJ), (ambience, NN), (just...   \n",
       "\n",
       "                                    Review_adjective  \\\n",
       "0                                [good, great, nice]   \n",
       "1                       [good, nice, next, friendly]   \n",
       "2  [crowded, candolim, perfect, commercial, allot...   \n",
       "3                             [candolim, nice, good]   \n",
       "4                                 [nice, good, good]   \n",
       "\n",
       "                                  Review_Common_Noun  \\\n",
       "0  [hotel, staffs, food, service, beach, shopping...   \n",
       "1  [hotel, service, vacation, family, friends, st...   \n",
       "2  [calangute, baga, palms, location, access, gat...   \n",
       "3                        [beach, food, buffet, pool]   \n",
       "4  [ambience, mins, resort, beach, rooms, food, p...   \n",
       "\n",
       "                               Review_Singular_Cnoun  \n",
       "0  [hotel, staff, food, service, beach, shopping,...  \n",
       "1  [hotel, service, vacation, family, friend, sta...  \n",
       "2  [calangute, baga, palm, location, acces, gate,...  \n",
       "3                        [beach, food, buffet, pool]  \n",
       "4  [ambience, min, resort, beach, room, food, pla...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLP Pre-Process\n",
    "df1 = entier_func(directory = r\"D:\\2.Praxis( all Stuff)\\3. subject wise records\\3.Term 3\\1.CAPP\\Data_File\"\n",
    "            ,data = \"10_hotels_reviews.csv\"\n",
    "            ,column_drop = ['Rating','hotel_name']\n",
    "            ,col_name = 'Review'\n",
    "            )\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Noun Lists: \n",
      "Showing the sample output:  ['hotel', 'staff', 'food', 'service', 'beach', 'shopping', 'market', 'vacation', 'family', 'friend']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aand</th>\n",
       "      <th>aasma</th>\n",
       "      <th>absence</th>\n",
       "      <th>absolute</th>\n",
       "      <th>academy</th>\n",
       "      <th>acasium</th>\n",
       "      <th>accent</th>\n",
       "      <th>accept</th>\n",
       "      <th>acces</th>\n",
       "      <th>accessibility</th>\n",
       "      <th>...</th>\n",
       "      <th>youngster</th>\n",
       "      <th>your</th>\n",
       "      <th>youre</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youve</th>\n",
       "      <th>yuck</th>\n",
       "      <th>yummy</th>\n",
       "      <th>zero</th>\n",
       "      <th>zomato</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6041</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6042</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6044</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6045</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6046 rows × 3271 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aand  aasma  absence  absolute  academy  acasium  accent  accept  acces  \\\n",
       "0        0      0        0         0        0        0       0       0      0   \n",
       "1        0      0        0         0        0        0       0       0      0   \n",
       "2        0      0        0         0        0        0       0       0      1   \n",
       "3        0      0        0         0        0        0       0       0      0   \n",
       "4        0      0        0         0        0        0       0       0      0   \n",
       "...    ...    ...      ...       ...      ...      ...     ...     ...    ...   \n",
       "6041     0      0        0         0        0        0       0       0      0   \n",
       "6042     0      0        0         0        0        0       0       0      0   \n",
       "6043     0      0        0         0        0        0       0       0      0   \n",
       "6044     0      0        0         0        0        0       0       0      0   \n",
       "6045     0      0        0         0        0        0       0       0      0   \n",
       "\n",
       "      accessibility  ...  youngster  your  youre  yourself  youve  yuck  \\\n",
       "0                 0  ...          0     0      0         0      0     0   \n",
       "1                 0  ...          0     0      0         0      0     0   \n",
       "2                 0  ...          0     0      0         0      0     0   \n",
       "3                 0  ...          0     0      0         0      0     0   \n",
       "4                 0  ...          0     0      0         0      0     0   \n",
       "...             ...  ...        ...   ...    ...       ...    ...   ...   \n",
       "6041              0  ...          0     0      0         0      0     0   \n",
       "6042              0  ...          0     0      0         0      0     0   \n",
       "6043              0  ...          0     0      0         0      0     0   \n",
       "6044              0  ...          0     0      0         0      0     0   \n",
       "6045              0  ...          0     0      0         0      0     0   \n",
       "\n",
       "      yummy  zero  zomato  zone  \n",
       "0         0     0       0     0  \n",
       "1         0     0       0     0  \n",
       "2         0     0       0     0  \n",
       "3         0     0       0     0  \n",
       "4         0     0       0     0  \n",
       "...     ...   ...     ...   ...  \n",
       "6041      0     0       0     0  \n",
       "6042      0     0       0     0  \n",
       "6043      0     0       0     0  \n",
       "6044      0     0       0     0  \n",
       "6045      0     0       0     0  \n",
       "\n",
       "[6046 rows x 3271 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary conversion\n",
    "data = binary_conversion(data='Review_Singular_Cnoun',colname = 'Review_Singular_Cnoun')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(area)</td>\n",
       "      <td>(room)</td>\n",
       "      <td>0.048627</td>\n",
       "      <td>0.468243</td>\n",
       "      <td>0.031426</td>\n",
       "      <td>0.646259</td>\n",
       "      <td>1.380176</td>\n",
       "      <td>0.008656</td>\n",
       "      <td>1.503235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(beach)</td>\n",
       "      <td>(breakfast)</td>\n",
       "      <td>0.217665</td>\n",
       "      <td>0.179788</td>\n",
       "      <td>0.047469</td>\n",
       "      <td>0.218085</td>\n",
       "      <td>1.213011</td>\n",
       "      <td>0.008336</td>\n",
       "      <td>1.048978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(breakfast)</td>\n",
       "      <td>(beach)</td>\n",
       "      <td>0.179788</td>\n",
       "      <td>0.217665</td>\n",
       "      <td>0.047469</td>\n",
       "      <td>0.264029</td>\n",
       "      <td>1.213011</td>\n",
       "      <td>0.008336</td>\n",
       "      <td>1.062998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(distance)</td>\n",
       "      <td>(beach)</td>\n",
       "      <td>0.043169</td>\n",
       "      <td>0.217665</td>\n",
       "      <td>0.035395</td>\n",
       "      <td>0.819923</td>\n",
       "      <td>3.766912</td>\n",
       "      <td>0.025999</td>\n",
       "      <td>4.344458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(beach)</td>\n",
       "      <td>(food)</td>\n",
       "      <td>0.217665</td>\n",
       "      <td>0.297056</td>\n",
       "      <td>0.081045</td>\n",
       "      <td>0.372340</td>\n",
       "      <td>1.253436</td>\n",
       "      <td>0.016387</td>\n",
       "      <td>1.119945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>(hotel, room, service)</td>\n",
       "      <td>(staff)</td>\n",
       "      <td>0.086338</td>\n",
       "      <td>0.357757</td>\n",
       "      <td>0.037711</td>\n",
       "      <td>0.436782</td>\n",
       "      <td>1.220888</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>1.140309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>(staff, room)</td>\n",
       "      <td>(hotel, service)</td>\n",
       "      <td>0.181277</td>\n",
       "      <td>0.137446</td>\n",
       "      <td>0.037711</td>\n",
       "      <td>0.208029</td>\n",
       "      <td>1.513531</td>\n",
       "      <td>0.012795</td>\n",
       "      <td>1.089123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>(staff, service)</td>\n",
       "      <td>(hotel, room)</td>\n",
       "      <td>0.101886</td>\n",
       "      <td>0.234535</td>\n",
       "      <td>0.037711</td>\n",
       "      <td>0.370130</td>\n",
       "      <td>1.578142</td>\n",
       "      <td>0.013815</td>\n",
       "      <td>1.215274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>(hotel, service)</td>\n",
       "      <td>(staff, room)</td>\n",
       "      <td>0.137446</td>\n",
       "      <td>0.181277</td>\n",
       "      <td>0.037711</td>\n",
       "      <td>0.274368</td>\n",
       "      <td>1.513531</td>\n",
       "      <td>0.012795</td>\n",
       "      <td>1.128290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>(room, service)</td>\n",
       "      <td>(staff, hotel)</td>\n",
       "      <td>0.164572</td>\n",
       "      <td>0.193516</td>\n",
       "      <td>0.037711</td>\n",
       "      <td>0.229146</td>\n",
       "      <td>1.184115</td>\n",
       "      <td>0.005864</td>\n",
       "      <td>1.046221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>415 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                antecedents       consequents  antecedent support  \\\n",
       "0                    (area)            (room)            0.048627   \n",
       "1                   (beach)       (breakfast)            0.217665   \n",
       "2               (breakfast)           (beach)            0.179788   \n",
       "3                (distance)           (beach)            0.043169   \n",
       "4                   (beach)            (food)            0.217665   \n",
       "..                      ...               ...                 ...   \n",
       "410  (hotel, room, service)           (staff)            0.086338   \n",
       "411           (staff, room)  (hotel, service)            0.181277   \n",
       "412        (staff, service)     (hotel, room)            0.101886   \n",
       "413        (hotel, service)     (staff, room)            0.137446   \n",
       "414         (room, service)    (staff, hotel)            0.164572   \n",
       "\n",
       "     consequent support   support  confidence      lift  leverage  conviction  \n",
       "0              0.468243  0.031426    0.646259  1.380176  0.008656    1.503235  \n",
       "1              0.179788  0.047469    0.218085  1.213011  0.008336    1.048978  \n",
       "2              0.217665  0.047469    0.264029  1.213011  0.008336    1.062998  \n",
       "3              0.217665  0.035395    0.819923  3.766912  0.025999    4.344458  \n",
       "4              0.297056  0.081045    0.372340  1.253436  0.016387    1.119945  \n",
       "..                  ...       ...         ...       ...       ...         ...  \n",
       "410            0.357757  0.037711    0.436782  1.220888  0.006823    1.140309  \n",
       "411            0.137446  0.037711    0.208029  1.513531  0.012795    1.089123  \n",
       "412            0.234535  0.037711    0.370130  1.578142  0.013815    1.215274  \n",
       "413            0.181277  0.037711    0.274368  1.513531  0.012795    1.128290  \n",
       "414            0.193516  0.037711    0.229146  1.184115  0.005864    1.046221  \n",
       "\n",
       "[415 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Association Rule\n",
    "rules = association_rule(data,dup_col1='antecedents',dup_col2='consequents')\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Max Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(area)</td>\n",
       "      <td>(room)</td>\n",
       "      <td>0.031426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(distance)</td>\n",
       "      <td>(beach)</td>\n",
       "      <td>0.035395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(place)</td>\n",
       "      <td>(beach)</td>\n",
       "      <td>0.038538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(resort)</td>\n",
       "      <td>(beach)</td>\n",
       "      <td>0.031591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(breakfast)</td>\n",
       "      <td>(food)</td>\n",
       "      <td>0.048462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>(room, service)</td>\n",
       "      <td>(hotel, location)</td>\n",
       "      <td>0.033907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>(hotel, service)</td>\n",
       "      <td>(room, location)</td>\n",
       "      <td>0.033907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>(room, location)</td>\n",
       "      <td>(staff, hotel)</td>\n",
       "      <td>0.039530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>(hotel, location)</td>\n",
       "      <td>(staff, room)</td>\n",
       "      <td>0.039530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>(hotel, service)</td>\n",
       "      <td>(staff, room)</td>\n",
       "      <td>0.037711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>329 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           antecedents        consequents   support\n",
       "0               (area)             (room)  0.031426\n",
       "1           (distance)            (beach)  0.035395\n",
       "2              (place)            (beach)  0.038538\n",
       "3             (resort)            (beach)  0.031591\n",
       "4          (breakfast)             (food)  0.048462\n",
       "..                 ...                ...       ...\n",
       "324    (room, service)  (hotel, location)  0.033907\n",
       "325   (hotel, service)   (room, location)  0.033907\n",
       "326   (room, location)     (staff, hotel)  0.039530\n",
       "327  (hotel, location)      (staff, room)  0.039530\n",
       "328   (hotel, service)      (staff, room)  0.037711\n",
       "\n",
       "[329 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the Max Support and removing the duplicates\n",
    "new_data = max_support_finding(rules,one_of_asso_column='support')\n",
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compactness Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['area', 'room'],\n",
       " ['distance', 'beach'],\n",
       " ['place', 'beach'],\n",
       " ['resort', 'beach'],\n",
       " ['breakfast', 'food'],\n",
       " ['breakfast', 'hotel'],\n",
       " ['breakfast', 'stay'],\n",
       " ['experience', 'food'],\n",
       " ['experience', 'hotel'],\n",
       " ['experience', 'room'],\n",
       " ['experience', 'service'],\n",
       " ['experience', 'staff'],\n",
       " ['facility', 'hotel'],\n",
       " ['facility', 'location'],\n",
       " ['facility', 'room'],\n",
       " ['facility', 'staff'],\n",
       " ['family', 'hotel'],\n",
       " ['family', 'room'],\n",
       " ['family', 'staff'],\n",
       " ['family', 'stay'],\n",
       " ['money', 'food'],\n",
       " ['place', 'food'],\n",
       " ['pool', 'food'],\n",
       " ['quality', 'food'],\n",
       " ['restaurant', 'food'],\n",
       " ['time', 'food'],\n",
       " ['market', 'hotel'],\n",
       " ['money', 'hotel'],\n",
       " ['night', 'hotel'],\n",
       " ['place', 'hotel'],\n",
       " ['pool', 'hotel'],\n",
       " ['quality', 'hotel'],\n",
       " ['restaurant', 'hotel'],\n",
       " ['time', 'hotel'],\n",
       " ['value', 'hotel'],\n",
       " ['water', 'hotel'],\n",
       " ['money', 'location'],\n",
       " ['place', 'location'],\n",
       " ['pool', 'location'],\n",
       " ['money', 'room'],\n",
       " ['money', 'staff'],\n",
       " ['night', 'room'],\n",
       " ['place', 'room'],\n",
       " ['place', 'service'],\n",
       " ['place', 'staff'],\n",
       " ['place', 'stay'],\n",
       " ['pool', 'room'],\n",
       " ['pool', 'service'],\n",
       " ['pool', 'staff'],\n",
       " ['pool', 'stay'],\n",
       " ['property', 'room'],\n",
       " ['quality', 'room'],\n",
       " ['resort', 'room'],\n",
       " ['restaurant', 'room'],\n",
       " ['restaurant', 'staff'],\n",
       " ['time', 'room'],\n",
       " ['value', 'room'],\n",
       " ['view', 'room'],\n",
       " ['water', 'room'],\n",
       " ['time', 'service'],\n",
       " ['time', 'staff'],\n",
       " ['breakfast', 'beach'],\n",
       " ['food', 'beach'],\n",
       " ['hotel', 'beach'],\n",
       " ['location', 'beach'],\n",
       " ['pool', 'beach'],\n",
       " ['room', 'beach'],\n",
       " ['service', 'beach'],\n",
       " ['staff', 'beach'],\n",
       " ['stay', 'beach'],\n",
       " ['location', 'breakfast'],\n",
       " ['pool', 'breakfast'],\n",
       " ['room', 'breakfast'],\n",
       " ['service', 'breakfast'],\n",
       " ['staff', 'breakfast'],\n",
       " ['food', 'hotel'],\n",
       " ['food', 'location'],\n",
       " ['food', 'room'],\n",
       " ['service', 'food'],\n",
       " ['food', 'staff'],\n",
       " ['food', 'stay'],\n",
       " ['location', 'hotel'],\n",
       " ['room', 'hotel'],\n",
       " ['service', 'hotel'],\n",
       " ['hotel', 'staff'],\n",
       " ['hotel', 'stay'],\n",
       " ['location', 'room'],\n",
       " ['service', 'location'],\n",
       " ['location', 'staff'],\n",
       " ['location', 'stay'],\n",
       " ['money', 'value'],\n",
       " ['service', 'room'],\n",
       " ['room', 'staff'],\n",
       " ['room', 'stay'],\n",
       " ['service', 'staff'],\n",
       " ['service', 'stay'],\n",
       " ['stay', 'staff']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compactness Pruning\n",
    "comp = compact_pruning()\n",
    "comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redundancy Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('area', 'room'): 5,\n",
       " ('distance', 'beach'): 54,\n",
       " ('place', 'beach'): 30,\n",
       " ('resort', 'beach'): 42,\n",
       " ('breakfast', 'food'): 65,\n",
       " ('breakfast', 'hotel'): 40,\n",
       " ('breakfast', 'stay'): 13,\n",
       " ('experience', 'food'): 18,\n",
       " ('experience', 'hotel'): 45,\n",
       " ('experience', 'room'): 13,\n",
       " ('experience', 'service'): 11,\n",
       " ('experience', 'staff'): 30,\n",
       " ('facility', 'hotel'): 11,\n",
       " ('facility', 'location'): 4,\n",
       " ('facility', 'room'): 4,\n",
       " ('facility', 'staff'): 9,\n",
       " ('family', 'hotel'): 18,\n",
       " ('family', 'room'): 14,\n",
       " ('family', 'staff'): 6,\n",
       " ('family', 'stay'): 33,\n",
       " ('money', 'food'): 9,\n",
       " ('place', 'food'): 25,\n",
       " ('pool', 'food'): 24,\n",
       " ('quality', 'food'): 44,\n",
       " ('restaurant', 'food'): 65,\n",
       " ('time', 'food'): 11,\n",
       " ('market', 'hotel'): 10,\n",
       " ('money', 'hotel'): 30,\n",
       " ('night', 'hotel'): 5,\n",
       " ('place', 'hotel'): 16,\n",
       " ('pool', 'hotel'): 33,\n",
       " ('quality', 'hotel'): 20,\n",
       " ('restaurant', 'hotel'): 30,\n",
       " ('time', 'hotel'): 46,\n",
       " ('value', 'hotel'): 27,\n",
       " ('water', 'hotel'): 23,\n",
       " ('money', 'location'): 7,\n",
       " ('place', 'location'): 16,\n",
       " ('pool', 'location'): 14,\n",
       " ('money', 'room'): 14,\n",
       " ('money', 'staff'): 6,\n",
       " ('night', 'room'): 7,\n",
       " ('place', 'room'): 11,\n",
       " ('place', 'service'): 16,\n",
       " ('place', 'staff'): 31,\n",
       " ('place', 'stay'): 125,\n",
       " ('pool', 'room'): 37,\n",
       " ('pool', 'service'): 17,\n",
       " ('pool', 'staff'): 41,\n",
       " ('pool', 'stay'): 13,\n",
       " ('property', 'room'): 6,\n",
       " ('quality', 'room'): 17,\n",
       " ('resort', 'room'): 16,\n",
       " ('restaurant', 'room'): 18,\n",
       " ('restaurant', 'staff'): 25,\n",
       " ('time', 'room'): 31,\n",
       " ('value', 'room'): 13,\n",
       " ('view', 'room'): 30,\n",
       " ('water', 'room'): 30,\n",
       " ('time', 'service'): 11,\n",
       " ('time', 'staff'): 20,\n",
       " ('breakfast', 'beach'): 7,\n",
       " ('food', 'beach'): 20,\n",
       " ('hotel', 'beach'): 158,\n",
       " ('location', 'beach'): 199,\n",
       " ('pool', 'beach'): 12,\n",
       " ('room', 'beach'): 19,\n",
       " ('service', 'beach'): 7,\n",
       " ('staff', 'beach'): 24,\n",
       " ('stay', 'beach'): 32,\n",
       " ('location', 'breakfast'): 45,\n",
       " ('pool', 'breakfast'): 33,\n",
       " ('room', 'breakfast'): 53,\n",
       " ('service', 'breakfast'): 44,\n",
       " ('staff', 'breakfast'): 58,\n",
       " ('food', 'hotel'): 94,\n",
       " ('food', 'location'): 54,\n",
       " ('food', 'room'): 80,\n",
       " ('service', 'food'): 115,\n",
       " ('food', 'staff'): 113,\n",
       " ('food', 'stay'): 25,\n",
       " ('location', 'hotel'): 203,\n",
       " ('room', 'hotel'): 93,\n",
       " ('service', 'hotel'): 87,\n",
       " ('hotel', 'staff'): 437,\n",
       " ('hotel', 'stay'): 190,\n",
       " ('location', 'room'): 74,\n",
       " ('service', 'location'): 33,\n",
       " ('location', 'staff'): 107,\n",
       " ('location', 'stay'): 30,\n",
       " ('money', 'value'): 8,\n",
       " ('service', 'room'): 79,\n",
       " ('room', 'staff'): 104,\n",
       " ('room', 'stay'): 41,\n",
       " ('service', 'staff'): 126,\n",
       " ('service', 'stay'): 19,\n",
       " ('stay', 'staff'): 84}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two Feature Counting\n",
    "twofeat = twofeats()\n",
    "twofeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'area': 215,\n",
       " 'beach': 1003,\n",
       " 'breakfast': 1235,\n",
       " 'distance': 205,\n",
       " 'experience': 482,\n",
       " 'facility': 119,\n",
       " 'family': 376,\n",
       " 'food': 1796,\n",
       " 'hotel': 3631,\n",
       " 'location': 1634,\n",
       " 'market': 215,\n",
       " 'money': 307,\n",
       " 'night': 138,\n",
       " 'place': 669,\n",
       " 'pool': 938,\n",
       " 'property': 322,\n",
       " 'quality': 400,\n",
       " 'resort': 413,\n",
       " 'restaurant': 396,\n",
       " 'room': 2363,\n",
       " 'service': 1429,\n",
       " 'staff': 2022,\n",
       " 'stay': 1727,\n",
       " 'time': 447,\n",
       " 'value': 408,\n",
       " 'view': 232,\n",
       " 'water': 425}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single Feature Counting\n",
    "sinfeat = single_feat()\n",
    "sinfeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'area': 210,\n",
       " 'beach': 399,\n",
       " 'breakfast': 877,\n",
       " 'distance': 151,\n",
       " 'experience': 365,\n",
       " 'facility': 91,\n",
       " 'family': 305,\n",
       " 'food': 1034,\n",
       " 'hotel': 2015,\n",
       " 'location': 848,\n",
       " 'market': 205,\n",
       " 'money': 233,\n",
       " 'night': 126,\n",
       " 'place': 399,\n",
       " 'pool': 714,\n",
       " 'property': 316,\n",
       " 'quality': 319,\n",
       " 'resort': 355,\n",
       " 'restaurant': 258,\n",
       " 'room': 1554,\n",
       " 'service': 864,\n",
       " 'staff': 801,\n",
       " 'stay': 1122,\n",
       " 'time': 328,\n",
       " 'value': 360,\n",
       " 'view': 202,\n",
       " 'water': 372}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the P-Support form (Single Feature - sum(Two Feature))\n",
    "p_support(twofeat,sinfeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
